{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6a880382",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FUNCTIONS_DIR: /Users/sam/Documents/Code/Planogram/uno/functions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Font file not found at fonts/Inter.ttf\n",
            "WARNING:root:Font file not found at fonts/Inter-Bold.ttf\n",
            "WARNING:root:Font file not found at fonts/Inter-Italic.ttf\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loading collection: app_machines\n",
            "app_machines slots cleaned: 8193 -> 8172 (removed 21, 0.26%)\n",
            "app_machines: 234 rows (machines themselves not filtered)\n",
            "Loading collection: products\n",
            "products (filtered unspecific rows): 2410 -> 1961 rows (removed 449, 18.63%)\n",
            "Loading collection: product_purchase_prices\n",
            "product_purchase_prices: 1370 -> 1370 rows (removed 0, 0.00%)\n",
            "Loading collection: product_nayax_mapping\n",
            "product_nayax_mapping: 5167 -> 5167 rows (removed 0, 0.00%)\n",
            "Loading historical sales from Parquet files...\n",
            "Historical sales (after filtering): 6979402 -> 5390446 rows (removed 1588956, 22.77%)\n",
            "Fetching latest sales data...\n",
            "Fetching sales after: 2025-11-18 23:59:50+00:00\n",
            "Enriching latest sales data...\n",
            "Cached machines data missing 'address' column (likely app_machines). Fetching full machines data...\n",
            "Fetching machines collection...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sam/Documents/Code/Planogram/uno/functions/planogram/data_loader.py:345: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  enriched_df[col] = enriched_df[col].fillna(False).astype(bool)\n",
            "/Users/sam/Documents/Code/Planogram/uno/functions/planogram/data_loader.py:345: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  enriched_df[col] = enriched_df[col].fillna(False).astype(bool)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded.\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure we are operating relative to the /functions directory\n",
        "FUNCTIONS_DIR = Path.cwd().parent\n",
        "sys.path.insert(0, str(FUNCTIONS_DIR))\n",
        "print(f'FUNCTIONS_DIR: {FUNCTIONS_DIR}')\n",
        "\n",
        "# Firebase setup\n",
        "from firebase_admin import initialize_app, credentials, get_app\n",
        "from google.cloud import firestore as google_firestore\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Use direct path to service account key inside /functions\n",
        "service_account_path = FUNCTIONS_DIR / 'serviceAccountKey.json'\n",
        "\n",
        "# Initialize Firebase Admin SDK\n",
        "try:\n",
        "    app = get_app()\n",
        "except ValueError:\n",
        "    cred = credentials.Certificate(service_account_path)\n",
        "    app = initialize_app(cred, {'storageBucket': 'uno-y-b48fb.appspot.com'})\n",
        "\n",
        "# Create Firestore client using the same credentials\n",
        "firestore_client = google_firestore.Client.from_service_account_json(str(service_account_path))\n",
        "\n",
        "# planogram imports\n",
        "from planogram import data_loader\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "firebase_data = data_loader.load_firebase_collections(firestore_client)  # Loads all collections\n",
        "sales_data, latest_sales_df = data_loader.load_all_sales_data(firestore_client, fetch_latest_sales=True, firebase_data=firebase_data) # Loads and combines historical + latest sales\n",
        "#sales_data = data_loader.enrich_with_purchase_prices(sales_data, firebase_data.get('product_purchase_prices'))\n",
        "print(\"Data loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "16413e0d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç DTYPE MISMATCH CHECK: Historical vs Latest\n",
            "------------------------------------------------------------\n",
            "‚ùå Mismatch in 'is_ICA_refiller':\n",
            "   Historical: object\n",
            "   Latest: bool\n",
            "\n",
            "‚ö†Ô∏è Found 1 column(s) with mismatched types.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def check_dtype_mismatch(df1, df2, df1_name=\"Historical\", df2_name=\"Latest\"):\n",
        "    \"\"\"\n",
        "    Checks for column data type mismatches between two DataFrames.\n",
        "    \"\"\"\n",
        "    print(f\"üîç DTYPE MISMATCH CHECK: {df1_name} vs {df2_name}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    common_cols = set(df1.columns) & set(df2.columns)\n",
        "    mismatches = 0\n",
        "    \n",
        "    for col in sorted(common_cols):\n",
        "        dtype1 = df1[col].dtype\n",
        "        dtype2 = df2[col].dtype\n",
        "        \n",
        "        # Handle nullable types vs numpy types (e.g., Int64 vs int64, float64 vs Float64)\n",
        "        # We consider them compatible if they represent the same kind of data\n",
        "        is_compatible = False\n",
        "        \n",
        "        if dtype1 == dtype2:\n",
        "            is_compatible = True\n",
        "        elif pd.api.types.is_integer_dtype(dtype1) and pd.api.types.is_integer_dtype(dtype2):\n",
        "            is_compatible = True\n",
        "        elif pd.api.types.is_float_dtype(dtype1) and pd.api.types.is_float_dtype(dtype2):\n",
        "            is_compatible = True\n",
        "        elif pd.api.types.is_string_dtype(dtype1) and pd.api.types.is_string_dtype(dtype2):\n",
        "            is_compatible = True\n",
        "        elif pd.api.types.is_object_dtype(dtype1) and pd.api.types.is_string_dtype(dtype2):\n",
        "             # Object often contains strings in pandas\n",
        "            is_compatible = True\n",
        "        elif pd.api.types.is_string_dtype(dtype1) and pd.api.types.is_object_dtype(dtype2):\n",
        "            is_compatible = True\n",
        "            \n",
        "        if not is_compatible:\n",
        "            print(f\"‚ùå Mismatch in '{col}':\")\n",
        "            print(f\"   {df1_name}: {dtype1}\")\n",
        "            print(f\"   {df2_name}: {dtype2}\")\n",
        "            mismatches += 1\n",
        "\n",
        "    if mismatches == 0:\n",
        "        print(\"‚úÖ No significant data type mismatches found.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Found {mismatches} column(s) with mismatched types.\")\n",
        "\n",
        "# Usage:\n",
        "check_dtype_mismatch(sales_data, latest_sales_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fc60d205",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DATA MAPPING EVALUATION REPORT\n",
            "==================================================\n",
            "üìä New Records Loaded: 148,502\n",
            "üìÖ Date Range: 2025-11-19 00:00:00+00:00 to 2025-12-11 00:00:00+00:00\n",
            "üîÑ Historical Data Ends: 2025-11-18 23:59:50+00:00\n",
            "‚úÖ Data is continuous (New starts after Old)\n",
            "\n",
            "--------------------------------------------------\n",
            "üß© ENRICHMENT SUCCESS RATES\n",
            "--------------------------------------------------\n",
            "üì¶ Product Mapping: 99.2% (147298/148502)\n",
            "   ‚ö†Ô∏è 211 unique 'nayax_name' values failed to map.\n",
            "   Top 5 unmapped: ['Powerking Proteinbar Choklad', 'Protein Smoothie', 'Dr Pepper 33cl', 'Mama Chin Curry Kyckling', 'S√§tra Fralla Kalkon&ost']\n",
            "ü§ñ Machine Mapping: 78.2% (116148/148502)\n",
            "   ‚ö†Ô∏è 165 unique 'machine_key' values failed to map.\n",
            "   Top 5 unmapped keys: ['Go Energy - Padel Enk√∂ping_268966842', 'Go Energy - S√ñS 1_450007630', 'Pro Ny - S√∂dert√§lje Akuten_418508875', 'Energi - Dalstorp padel _341395781', 'Go Energy -  S√ñS 3_450007629']\n",
            "\n",
            "--------------------------------------------------\n",
            "üîç COLUMN CONSISTENCY\n",
            "--------------------------------------------------\n",
            "‚úÖ Columns match perfectly between Historical and Latest.\n",
            "\n",
            "--------------------------------------------------\n",
            "üõë NULL VALUE CHECK (Critical Columns)\n",
            "--------------------------------------------------\n",
            "Column               | Null Count | Null %    \n",
            "----------------------------------------------\n",
            "category             | 41526      | 28.0     %\n",
            "ean                  | 46465      | 31.3     %\n",
            "purchase_price_kr    | 44600      | 30.0     %\n",
            "machine_group_tag    | 3029       | 2.0      %\n",
            "refiller             | 0          | 0.0      %\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_data_mapping(latest_df, historical_df):\n",
        "    print(\"=\"*50)\n",
        "    print(\"DATA MAPPING EVALUATION REPORT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if latest_df.empty:\n",
        "        print(\"‚ùå Latest sales DataFrame is empty! Nothing to evaluate.\")\n",
        "        return\n",
        "\n",
        "    # 1. Basic Stats\n",
        "    print(f\"üìä New Records Loaded: {len(latest_df):,}\")\n",
        "    print(f\"üìÖ Date Range: {latest_df['local_timestamp'].min()} to {latest_df['local_timestamp'].max()}\")\n",
        "    \n",
        "    # Check continuity with historical data\n",
        "    if not historical_df.empty:\n",
        "        last_hist_date = historical_df['local_timestamp'].max()\n",
        "        print(f\"üîÑ Historical Data Ends: {last_hist_date}\")\n",
        "        if latest_df['local_timestamp'].min() > last_hist_date:\n",
        "            print(\"‚úÖ Data is continuous (New starts after Old)\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Overlap detected! {len(latest_df[latest_df['local_timestamp'] <= last_hist_date])} records overlap.\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"üß© ENRICHMENT SUCCESS RATES\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 2. Product Enrichment (Mapping nayax_name -> product_name)\n",
        "    total_rows = len(latest_df)\n",
        "    mapped_products = latest_df['product_name'].notna().sum()\n",
        "    pct_products = (mapped_products / total_rows) * 100\n",
        "    \n",
        "    print(f\"üì¶ Product Mapping: {pct_products:.1f}% ({mapped_products}/{total_rows})\")\n",
        "    \n",
        "    if pct_products < 100:\n",
        "        missing_products = latest_df[latest_df['product_name'].isna()]['nayax_name'].unique()\n",
        "        print(f\"   ‚ö†Ô∏è {len(missing_products)} unique 'nayax_name' values failed to map.\")\n",
        "        print(f\"   Top 5 unmapped: {list(missing_products[:5])}\")\n",
        "\n",
        "    # 3. Machine Enrichment (Mapping machine_key -> address/machine_model)\n",
        "    # Using 'address' as a proxy for successful machine enrichment\n",
        "    mapped_machines = latest_df['address'].notna().sum()\n",
        "    pct_machines = (mapped_machines / total_rows) * 100\n",
        "    \n",
        "    print(f\"ü§ñ Machine Mapping: {pct_machines:.1f}% ({mapped_machines}/{total_rows})\")\n",
        "    \n",
        "    if pct_machines < 100:\n",
        "        missing_machines = latest_df[latest_df['address'].isna()]['machine_key'].unique()\n",
        "        print(f\"   ‚ö†Ô∏è {len(missing_machines)} unique 'machine_key' values failed to map.\")\n",
        "        print(f\"   Top 5 unmapped keys: {list(missing_machines[:5])}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"üîç COLUMN CONSISTENCY\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 4. Column Comparison\n",
        "    hist_cols = set(historical_df.columns)\n",
        "    new_cols = set(latest_df.columns)\n",
        "    \n",
        "    missing_in_new = hist_cols - new_cols\n",
        "    extra_in_new = new_cols - hist_cols\n",
        "    \n",
        "    if not missing_in_new and not extra_in_new:\n",
        "        print(\"‚úÖ Columns match perfectly between Historical and Latest.\")\n",
        "    else:\n",
        "        if missing_in_new:\n",
        "            print(f\"‚ùå Missing columns in Latest: {missing_in_new}\")\n",
        "        if extra_in_new:\n",
        "            print(f\"‚ÑπÔ∏è Extra columns in Latest: {extra_in_new}\")\n",
        "\n",
        "    # 5. Null Value Check for Critical Columns\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(\"üõë NULL VALUE CHECK (Critical Columns)\")\n",
        "    print(\"-\" * 50)\n",
        "    critical_cols = ['category', 'ean', 'purchase_price_kr', 'machine_group_tag', 'refiller']\n",
        "    \n",
        "    print(f\"{'Column':<20} | {'Null Count':<10} | {'Null %':<10}\")\n",
        "    print(\"-\" * 46)\n",
        "    for col in critical_cols:\n",
        "        if col in latest_df.columns:\n",
        "            null_count = latest_df[col].isna().sum()\n",
        "            null_pct = (null_count / total_rows) * 100\n",
        "            print(f\"{col:<20} | {null_count:<10} | {null_pct:<9.1f}%\")\n",
        "        else:\n",
        "            print(f\"{col:<20} | {'MISSING':<10} | -\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_data_mapping(latest_sales_df, sales_data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
